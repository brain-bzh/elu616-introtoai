{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Artificial Intelligence - TP1 - October 24th 2018 \n",
    "--\n",
    "\n",
    "At the end of this session, you will be able to : \n",
    "- Perform basic supervised learning tasks using sklearn\n",
    "- Generate PyRat Datasets for a supervised learning setting\n",
    "- Apply supervised learning on PyRat datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tqdm package is useful to visualize progress with long computations. \n",
    "# Install it using pip \n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import ast\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1 - Basics of machine learning using sklearn\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn is a very powerful package that implements most machine learning methods. sklearn also includes cross-validation procedures in order to prevent overfitting, many useful metrics and data manipulation techniques that enables very careful experimentations with machine learning. It is also very straightforward to use. We will introduce a few basic concepts of sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, it is very easy to simulate data with sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function make_blobs to generate clouds of points with $d=2$, and visualize them using the function *scatter* from matplotlib.pyplot. You can generate as many samples as you want.\n",
    "\n",
    "You can generate several clouds of points using the argument centers. We recommend using random_state=0 so that your results are from the same distribution from our tests \n",
    "\n",
    "Vocabulary : n_samples is the number of generated samples, n_features is $d$ (number of dimensions), centers are the number of classes. \n",
    "\n",
    "hint : you can use the output \"y\" as an argument for the color argument (\"c\") of the scatter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED - generate blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED -  plot\n",
    "### Don't forget to import pyplot and use %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the other arguments from make_blobs in order to change the variance of the blobs, or the coordinates of their center. You can also experiment in higher dimension, although it becomes difficult to visualize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn has many other data generators, as well as ways to load standard datasets of various sizes. Check them out here : \n",
    "\n",
    "http://scikit-learn.org/stable/datasets/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated a simple dataset, let's try to do a basic supervised learning approach. \n",
    "\n",
    "First, in order to mesure the model capability of generalizing, we will have to split the dataset into a training set and a test set. This split is also called cross validation. The test set is a part of your dataset that your model will not see during the training and will be used as a proxy for your \"real world\" examples.\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/1600/1*u03UsvBGwkYD4E7BObpcaw.png\"></center>\n",
    "<center><small>Image taken from https://towardsdatascience.com/machine-learning-workflow-on-diabetes-data-part-01-573864fcc6b8</small></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has functions to do so, with parameters controlling how to split the dataset so we are going to be showing how you should do. \n",
    "\n",
    "Use the train_test_split function in order to generate x_train,x_test, y_train, y_test, from the blobs we just generated, here we split the dataset in 80% train and 20% test. We use random_state = 0 so that the function always returns the same split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#### CELL TO BE COMPLETED "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the shapes of the generated vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape,x_test.shape,x_blobs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a K-Nearest Neighbor classifier to test whether we can classify this data. Create a <b>classifier</b>, train it using your <b> training set </b> and evaluate it by its <b>accuracy</b> on both <b>the train and test sets</b>. \n",
    "\n",
    "A k-nearest neighbor classifier (also known as KNN) is a method where for each object that you want to predict the class you look at the K (an hyperparameter) nearest examples from the training (using a distance metric, in our case the euclidean distance). This object is then classified by a majority vote from those neighbors, in other words the object is predicted with the most common class among its neighbours.\n",
    "\n",
    "To use a Nearest Neighbor with sklearn you have to use the class [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).\n",
    "\n",
    "The sklearn API is consistent between the methods. This means that for almost every method they propose you can train it using [object.fit](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.fit), you can use it to make prediction with [object.predict](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.predict) and finally verify the <b>accuracy</b> of the method using [object.score](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 1\n",
    "classifier = KNeighborsClassifier(n_neighbors=k,n_jobs=1)\n",
    "\n",
    "### CELL TO BE COMPLETED - train the classifier and get the accuracy in both sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your classifier should have a train accuracy of 1, while the test accuracy should be high but not perfect.\n",
    "\n",
    "This is caused by the bias-variance trade-off. The 1NN classifier always has a bias of 0 (it perfectly classifies the training set) but it should have a high variance given that having one more example in the training set can completely change a decision.\n",
    "\n",
    "To try to avoid having such a high variance, test different values of k and plot the accuracies given the different values of the hyperparameter k. \n",
    "\n",
    "If you have time we advise you to do the same analysis but varying the train/test split size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = list()\n",
    "test_acc = list() # list to add the test set accuracies\n",
    "test_ks = range(1,25)# list containing values of k to be tested\n",
    "\n",
    "# CELL TO BE COMPLETED - Train networks with varying k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the classifier trained, bias-variance analysed, it is now time to look at other metrics based on your results. It is important to remember that accuracy is a key metric, but it is not the <b> only </b> metric you should be focusing on.\n",
    "\n",
    "We are going to be printing a [classification report](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report) and the [confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) for both the training and test sets\n",
    "\n",
    "In the classification report we are going to see 3 new metrics. They are really important because the accuracy does not show a complete portrait of your results.\n",
    "\n",
    "* Precision: What is the percentage of cases that your model was correct while predicting the given class\n",
    "* Recall: What is the percentage of cases that your model was correct when the example was a member of the given class.\n",
    "* F1 Score: The harmonic mean from precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "y_pred_train = classifier.predict(x_train)\n",
    "report = classification_report(y_true=y_train,y_pred=y_pred_train)\n",
    "matrix = confusion_matrix(y_true=y_train,y_pred=y_pred_train)\n",
    "print(\"Training Set:\")\n",
    "print(report)\n",
    "print(matrix)\n",
    "plt.matshow(matrix)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Real class\")\n",
    "plt.ylabel(\"Predicted class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED - Generate the report and confusion matrix for the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are going to plot the decision boundaries from our model. For this you should use the function plot_boundaries given below. You can only do this if the tensor representing your data is two dimensional.\n",
    "\n",
    "This function will be testing our model with values ranging from the smallest x to the highest x and from the lowest y to the highest y each varying by $h$ and plotting it nicely. [Link to the original implementation](http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "def plot_boundaries(classifier,X,Y,h=0.2):\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    x0, x1 = np.meshgrid(np.arange(x0_min, x0_max,h),\n",
    "                         np.arange(x1_min, x1_max,h))\n",
    "    dataset = np.c_[x0.ravel(),x1.ravel()]\n",
    "    Z = classifier.predict(dataset)\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(x0.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(x0, x1, Z)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=Y,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(x0.min(), x0.max())\n",
    "    plt.ylim(x1.min(), x1.max())\n",
    "plot_boundaries(classifier,x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2 - Generating PyRat datasets\n",
    "--\n",
    "First and foremost you need the latest version of PyRat. To do that, just clone the [official PyRat repository](https://github.com/vgripon/pyrat). \n",
    "\n",
    "Syntax is \"git clone repo destinationdir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO : open a terminal tab / window and clone the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now launch Pyrat Games. \n",
    "\n",
    "In the context of the AI course, we are going to simplify the rules of PyRat a bit.\n",
    "In fact, we are going to remove all walls and mud penalties. Also, we are not going to consider symmetric mazes anymore.\n",
    "\n",
    "As such, a default game would be obtained with the following parameters:\n",
    "<pre>python3 pyrat.py -p 40 -md 0 -d 0 --nonsymmetric</pre>\n",
    "\n",
    "In the supervised and unsupervised projects, we are going to obtain data while looking at plays between two greedy algorithms. Generating 1000 such games while saving data is easily obtained with PyRat by using:\n",
    "<pre>python3 pyrat.py -p 40 -md 0 -d 0 --nonsymmetric --rat AIs/manh.py --python AIs/manh.py --tests 1000 --nodrawing --synchronous --save</pre>\n",
    "\n",
    "We recommend that you open another Terminal to launch this command, because generating 1000 games will take a few minutes, and you won't be able to evaluate any other cell from the jupyter notebook. \n",
    "\n",
    "It is possible to open a terminal window from the \"Home\" Interface of Jupyter Notebook.\n",
    "\n",
    "PS: If you don't have pygame installed in your machine you can open a terminal and run\n",
    "\n",
    "<pre>pip install pygame</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1000 generated games will be in the \"saves\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO : open a terminal tab / window and launch the command to generate the games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the games into numpy arrays, we will make use of a few functions that we define here. Feel try to modify it later to your own needs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mazeHeight = 15\n",
    "mazeWidth = 21\n",
    "\n",
    "\n",
    "def convert_input(maze, mazeWidth, mazeHeight, piecesOfCheese):\n",
    "    im_size = (mazeWidth, mazeHeight) \n",
    "    canvas = np.zeros(im_size,dtype=np.int8)\n",
    "    for (x_cheese,y_cheese) in piecesOfCheese:\n",
    "        canvas[x_cheese,y_cheese] = 1\n",
    "    # For use with sklearn, we flatten the matrix into an vector\n",
    "    return canvas.ravel()\n",
    "\n",
    "PHRASES = {\n",
    "    \"# Random seed\\n\": \"seed\",\n",
    "    \"# MazeMap\\n\": \"maze\",\n",
    "    \"# Pieces of cheese\\n\": \"pieces\"    ,\n",
    "    \"# Rat initial location\\n\": \"rat\"    ,\n",
    "    \"# Python initial location\\n\": \"python\"   , \n",
    "    \"rat_location then python_location then pieces_of_cheese then rat_decision then python_decision\\n\": \"play\"\n",
    "}\n",
    " \n",
    "MOVE_DOWN = 'D'\n",
    "MOVE_LEFT = 'L'\n",
    "MOVE_RIGHT = 'R'\n",
    "MOVE_UP = 'U'\n",
    " \n",
    "translate_action = {\n",
    "    MOVE_LEFT:0,\n",
    "    MOVE_RIGHT:1,\n",
    "    MOVE_UP:2,\n",
    "    MOVE_DOWN:3\n",
    "}\n",
    " \n",
    "def process_file(filename):\n",
    "    f = open(filename,\"r\")    \n",
    "    info = f.readline()\n",
    "    params = dict(play=list())\n",
    "    while info is not None:\n",
    "        if info.startswith(\"{\"):\n",
    "            params[\"end\"] = ast.literal_eval(info)\n",
    "            break\n",
    "        if \"turn \" in info:\n",
    "            info = info[info.find('rat_location'):]\n",
    "        if info in PHRASES.keys():\n",
    "            param = PHRASES[info]\n",
    "            if param == \"play\":\n",
    "                rat = ast.literal_eval(f.readline())\n",
    "                python = ast.literal_eval(f.readline())\n",
    "                pieces = ast.literal_eval(f.readline())\n",
    "                rat_decision = f.readline().replace(\"\\n\",\"\")\n",
    "                python_decision = f.readline().replace(\"\\n\",\"\")\n",
    "                play_dict = dict(\n",
    "                    rat=rat,python=python,piecesOfCheese=pieces,\n",
    "                    rat_decision=rat_decision,python_decision=python_decision)\n",
    "                params[param].append(play_dict)\n",
    "            else:\n",
    "                params[param] = ast.literal_eval(f.readline())\n",
    "        else:\n",
    "            print(\"did not understand:\", info)\n",
    "            break\n",
    "        info = f.readline()\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to parse the \"saves\" folder in order to generate the data into a numpy array. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = list()\n",
    "directory = \"put the directory of the games\"\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in tqdm.tqdm(files):\n",
    "        try:\n",
    "            if filename.startswith(\".\"):\n",
    "                continue\n",
    "            game_params = process_file(directory+filename)\n",
    "            games.append(game_params)\n",
    "        except:\n",
    "            print(\"Filename {} did not work\".format(filename))\n",
    "\n",
    "x = np.array([]).reshape(0,mazeWidth * mazeHeight)\n",
    "y = np.array([]).reshape(0,1)\n",
    "wins_python = 0\n",
    "wins_rat = 0\n",
    "for game in tqdm.tqdm(games):\n",
    "    if game[\"end\"][\"win_python\"] == 1: \n",
    "        wins_python += 1\n",
    "    elif game[\"end\"][\"win_rat\"] == 1:\n",
    "        wins_rat += 1    \n",
    "    canvas = convert_input(game[\"maze\"], mazeWidth, mazeHeight, game[\"play\"][0][\"piecesOfCheese\"])\n",
    "    if game[\"end\"][\"win_python\"] == 1:\n",
    "        y = np.append(y,1)\n",
    "    elif game[\"end\"][\"win_rat\"] == 1:\n",
    "        y = np.append(y,-1)\n",
    "    else:\n",
    "        y = np.append(y,0)\n",
    "    x = np.concatenate([x, canvas.reshape(1,-1)], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x and y are numpy array, feel free to save them to a .npz file as seen in TP0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CELL TO BE COMPLETED\n",
    "### CHECK THE SHAPES OF X AND Y \n",
    "### SAVE X AND Y IN A NPZ FILE \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3 - Application to PyRat Datasets\n",
    "--\n",
    "\n",
    "Now it is your turn, generate a pyrat dataset, load it in the notebook and evaluate a KNN classifier using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complet the following cells by using the X and y vectors generated from the PyRat Games, and test supervised learning using a KNN classifier. \n",
    "\n",
    "Evaluate this simple solution using the classification report and confusion matrices as done above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the classification report and try to interpret it. Can you figure out why we obtain such scores ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4 - Further tests on PyRat\n",
    "--\n",
    "If you finished part 3, you will have noticed that the performances are very limited, especially when trying to classify the \"draw\" games. \n",
    "\n",
    "In this last part, we suggest you test the following: \n",
    "\n",
    "- Trying to make a binary classification between Pyrat and python wins\n",
    "- Trying to improve the classification performance of draw class by using a balanced dataset for the three classes (i.e. try to have the same number of examples in each class)\n",
    "\n",
    "To go further:\n",
    "- Trying to change the parameters of the maze such as, mazeWidth, mazeHeight, piecesOfCheese,... \n",
    "\n",
    "\n",
    "N.b. When working on your project P1, we expect you to investigate these last questions in order to explore the method you chose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
